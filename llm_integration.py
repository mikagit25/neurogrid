#!/usr/bin/env python3
"""
NeuroGrid LLM Integration Service
–ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–µ LLM –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ —Ä–∞–∑–ª–∏—á–Ω—ã–µ API
"""

import asyncio
import aiohttp
import json
import os
from datetime import datetime
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class LLMProvider:
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤ LLM"""
    
    def __init__(self, name, api_key=None):
        self.name = name
        self.api_key = api_key
        self.session = None
        
    async def initialize(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞"""
        self.session = aiohttp.ClientSession()
        
    async def cleanup(self):
        """–û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤"""
        if self.session:
            await self.session.close()
            
    async def generate(self, prompt, model=None, **kwargs):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏"""
        raise NotImplementedError("Subclasses must implement generate method")

class OpenAIProvider(LLMProvider):
    """–ü—Ä–æ–≤–∞–π–¥–µ—Ä –¥–ª—è OpenAI API"""
    
    def __init__(self, api_key=None):
        super().__init__("OpenAI", api_key or os.getenv('OPENAI_API_KEY'))
        self.base_url = "https://api.openai.com/v1"
        
    async def generate(self, prompt, model="gpt-3.5-turbo", **kwargs):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç —á–µ—Ä–µ–∑ OpenAI API"""
        if not self.api_key:
            return {"error": "OpenAI API key not provided", "fallback": True}
            
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        data = {
            "model": model,
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": kwargs.get('max_tokens', 1000),
            "temperature": kwargs.get('temperature', 0.7)
        }
        
        try:
            async with self.session.post(
                f"{self.base_url}/chat/completions",
                json=data,
                headers=headers
            ) as response:
                if response.status == 200:
                    result = await response.json()
                    return {
                        "result": result["choices"][0]["message"]["content"],
                        "model": model,
                        "provider": "OpenAI",
                        "tokens_used": result.get("usage", {}).get("total_tokens", 0)
                    }
                else:
                    error_text = await response.text()
                    return {"error": f"OpenAI API error: {response.status}", "details": error_text, "fallback": True}
                    
        except Exception as e:
            logger.error(f"OpenAI API error: {e}")
            return {"error": str(e), "fallback": True}

class HuggingFaceProvider(LLMProvider):
    """–ü—Ä–æ–≤–∞–π–¥–µ—Ä –¥–ª—è Hugging Face API"""
    
    def __init__(self, api_key=None):
        super().__init__("HuggingFace", api_key or os.getenv('HUGGINGFACE_API_KEY'))
        self.base_url = "https://api-inference.huggingface.co/models"
        
    async def generate(self, prompt, model="microsoft/DialoGPT-medium", **kwargs):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç —á–µ—Ä–µ–∑ Hugging Face API"""
        if not self.api_key:
            return {"error": "Hugging Face API key not provided", "fallback": True}
            
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        data = {
            "inputs": prompt,
            "parameters": {
                "max_new_tokens": kwargs.get('max_tokens', 500),
                "temperature": kwargs.get('temperature', 0.7),
                "return_full_text": False
            }
        }
        
        try:
            async with self.session.post(
                f"{self.base_url}/{model}",
                json=data,
                headers=headers
            ) as response:
                if response.status == 200:
                    result = await response.json()
                    if isinstance(result, list) and len(result) > 0:
                        return {
                            "result": result[0].get("generated_text", "No response generated"),
                            "model": model,
                            "provider": "HuggingFace"
                        }
                    else:
                        return {"error": "Unexpected response format", "fallback": True}
                else:
                    error_text = await response.text()
                    return {"error": f"HuggingFace API error: {response.status}", "details": error_text, "fallback": True}
                    
        except Exception as e:
            logger.error(f"HuggingFace API error: {e}")
            return {"error": str(e), "fallback": True}

class LocalLLMProvider(LLMProvider):
    """–ü—Ä–æ–≤–∞–π–¥–µ—Ä –¥–ª—è –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (—á–µ—Ä–µ–∑ Ollama –∏–ª–∏ –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ–µ)"""
    
    def __init__(self, base_url="http://localhost:11434"):
        super().__init__("Local")
        self.base_url = base_url
        
    async def generate(self, prompt, model="llama2", **kwargs):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç —á–µ—Ä–µ–∑ –ª–æ–∫–∞–ª—å–Ω—ã–π API"""
        data = {
            "model": model,
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": kwargs.get('temperature', 0.7),
                "num_predict": kwargs.get('max_tokens', 500)
            }
        }
        
        try:
            async with self.session.post(
                f"{self.base_url}/api/generate",
                json=data
            ) as response:
                if response.status == 200:
                    result = await response.json()
                    return {
                        "result": result.get("response", "No response generated"),
                        "model": model,
                        "provider": "Local",
                        "context": result.get("context")
                    }
                else:
                    return {"error": f"Local API error: {response.status}", "fallback": True}
                    
        except Exception as e:
            logger.error(f"Local API error: {e}")
            return {"error": str(e), "fallback": True}

class MockLLMProvider(LLMProvider):
    """–ú–æ–∫ –ø—Ä–æ–≤–∞–π–¥–µ—Ä –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏"""
    
    def __init__(self):
        super().__init__("Mock")
        
    async def initialize(self):
        """–ú–æ–∫ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è"""
        pass
        
    async def cleanup(self):
        """–ú–æ–∫ –æ—á–∏—Å—Ç–∫–∞"""
        pass
        
    async def generate(self, prompt, model="mock-llm", **kwargs):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –º–æ–∫ –æ—Ç–≤–µ—Ç"""
        # –°–∏–º—É–ª–∏—Ä—É–µ–º –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        await asyncio.sleep(1)
        
        responses = [
            f"This is a mock response to your prompt: '{prompt[:100]}...' Generated by NeuroGrid's mock LLM provider for demonstration purposes.",
            f"Mock AI Response: I understand you're asking about '{prompt[:50]}...'. In a production setup, this would be processed by a real language model.",
            f"NeuroGrid Mock LLM: Your input '{prompt[:75]}...' has been processed. This demonstrates the system's ability to route requests to available language models.",
            f"Simulated Response: Based on your query '{prompt[:60]}...', here's what a real LLM would provide in a production NeuroGrid deployment."
        ]
        
        import random
        return {
            "result": random.choice(responses),
            "model": model,
            "provider": "Mock",
            "tokens_used": len(prompt.split()) + random.randint(50, 200)
        }

class LLMIntegrationService:
    """–û—Å–Ω–æ–≤–Ω–æ–π —Å–µ—Ä–≤–∏—Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ LLM"""
    
    def __init__(self, coordinator_url="http://localhost:8080"):
        self.coordinator_url = coordinator_url
        self.providers = {}
        self.session = None
        self.running = False
        
    async def initialize(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–∞"""
        logger.info("ü§ñ Initializing LLM Integration Service...")
        
        self.session = aiohttp.ClientSession()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã
        await self.setup_providers()
        
        # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º—Å—è –∫–∞–∫ LLM —Å–µ—Ä–≤–∏—Å
        await self.register_service()
        
    async def setup_providers(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤"""
        logger.info("‚öôÔ∏è Setting up LLM providers...")
        
        # –í—Å–µ–≥–¥–∞ –¥–æ—Å—Ç—É–ø–Ω—ã–π –º–æ–∫ –ø—Ä–æ–≤–∞–π–¥–µ—Ä
        mock_provider = MockLLMProvider()
        await mock_provider.initialize()
        self.providers["mock"] = mock_provider
        logger.info("‚úÖ Mock LLM provider ready")
        
        # OpenAI –ø—Ä–æ–≤–∞–π–¥–µ—Ä (–µ—Å–ª–∏ –µ—Å—Ç—å API –∫–ª—é—á)
        if os.getenv('OPENAI_API_KEY'):
            openai_provider = OpenAIProvider()
            await openai_provider.initialize()
            self.providers["openai"] = openai_provider
            logger.info("‚úÖ OpenAI provider ready")
        else:
            logger.info("‚ö†Ô∏è OpenAI API key not found - provider not initialized")
            
        # HuggingFace –ø—Ä–æ–≤–∞–π–¥–µ—Ä (–µ—Å–ª–∏ –µ—Å—Ç—å API –∫–ª—é—á)
        if os.getenv('HUGGINGFACE_API_KEY'):
            hf_provider = HuggingFaceProvider()
            await hf_provider.initialize()
            self.providers["huggingface"] = hf_provider
            logger.info("‚úÖ HuggingFace provider ready")
        else:
            logger.info("‚ö†Ô∏è HuggingFace API key not found - provider not initialized")
            
        # –õ–æ–∫–∞–ª—å–Ω—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä (–ø—ã—Ç–∞–µ–º—Å—è –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è)
        try:
            local_provider = LocalLLMProvider()
            await local_provider.initialize()
            # –¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏
            async with local_provider.session.get("http://localhost:11434/api/tags") as response:
                if response.status == 200:
                    self.providers["local"] = local_provider
                    logger.info("‚úÖ Local LLM provider ready (Ollama detected)")
                else:
                    await local_provider.cleanup()
        except:
            logger.info("‚ö†Ô∏è Local LLM provider not available (Ollama not running)")
            
        logger.info(f"üìä Total providers initialized: {len(self.providers)}")
        
    async def register_service(self):
        """–†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ—Ç —Å–µ—Ä–≤–∏—Å –≤ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä–µ"""
        registration_data = {
            "service_type": "llm_integration",
            "providers": list(self.providers.keys()),
            "capabilities": ["text-generation", "chat", "completion"],
            "status": "online"
        }
        
        try:
            async with self.session.post(
                f"{self.coordinator_url}/api/services/register",
                json=registration_data
            ) as response:
                if response.status == 200:
                    logger.info("‚úÖ LLM Integration Service registered")
                else:
                    logger.warning(f"‚ö†Ô∏è Registration returned {response.status}")
        except Exception as e:
            logger.error(f"‚ùå Service registration failed: {e}")
            
    async def process_request(self, prompt, model=None, provider=None, **kwargs):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∑–∞–ø—Ä–æ—Å –∫ LLM"""
        logger.info(f"üìù Processing request: '{prompt[:50]}...'")
        
        # –í—ã–±–∏—Ä–∞–µ–º –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
        if provider and provider in self.providers:
            selected_provider = self.providers[provider]
        else:
            # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –ª—É—á—à–µ–≥–æ –¥–æ—Å—Ç—É–ø–Ω–æ–≥–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
            if "openai" in self.providers:
                selected_provider = self.providers["openai"]
            elif "huggingface" in self.providers:
                selected_provider = self.providers["huggingface"]
            elif "local" in self.providers:
                selected_provider = self.providers["local"]
            else:
                selected_provider = self.providers["mock"]
                
        logger.info(f"üéØ Using provider: {selected_provider.name}")
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
        start_time = datetime.now()
        result = await selected_provider.generate(prompt, model, **kwargs)
        processing_time = (datetime.now() - start_time).total_seconds()
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å –æ—à–∏–±–∫–∞ –∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è fallback, –∏—Å–ø–æ–ª—å–∑—É–µ–º –º–æ–∫
        if result.get("error") and result.get("fallback") and selected_provider.name != "Mock":
            logger.warning(f"‚ö†Ô∏è Fallback to mock provider due to error: {result['error']}")
            result = await self.providers["mock"].generate(prompt, model, **kwargs)
            
        result["processing_time"] = processing_time
        result["timestamp"] = datetime.now().isoformat()
        
        logger.info(f"‚úÖ Request processed in {processing_time:.2f}s")
        return result
        
    async def start_service(self):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç —Å–µ—Ä–≤–∏—Å"""
        logger.info("üöÄ Starting LLM Integration Service...")
        self.running = True
        
        # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É –∑–∞–¥–∞—á –∏–∑ –æ—á–µ—Ä–µ–¥–∏
        # –ü–æ–∫–∞ —á—Ç–æ —Å–µ—Ä–≤–∏—Å –≥–æ—Ç–æ–≤ –ø—Ä–∏–Ω–∏–º–∞—Ç—å –ø—Ä—è–º—ã–µ –∑–∞–ø—Ä–æ—Å—ã
        
        try:
            while self.running:
                # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ø—Ä–æ–≤–µ—Ä–∫—É –æ—á–µ—Ä–µ–¥–∏ –∑–∞–¥–∞—á
                await asyncio.sleep(1)
        except KeyboardInterrupt:
            logger.info("‚ö° Received shutdown signal")
        finally:
            await self.cleanup()
            
    async def cleanup(self):
        """–û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤"""
        logger.info("üßπ Cleaning up LLM Integration Service...")
        
        for provider in self.providers.values():
            await provider.cleanup()
            
        if self.session:
            await self.session.close()
            
        logger.info("üëã LLM Integration Service stopped")

# –¢–µ—Å—Ç–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è
async def test_llm_integration():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é LLM"""
    print("üß™ Testing LLM Integration Service")
    print("=" * 50)
    
    service = LLMIntegrationService()
    await service.initialize()
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
    test_prompts = [
        "What is artificial intelligence?",
        "Explain how decentralized computing works",
        "Write a short poem about technology"
    ]
    
    for prompt in test_prompts:
        print(f"\nüìù Testing prompt: '{prompt}'")
        result = await service.process_request(prompt)
        
        print(f"ü§ñ Provider: {result.get('provider', 'Unknown')}")
        print(f"‚è±Ô∏è Processing time: {result.get('processing_time', 0):.2f}s")
        if result.get('error'):
            print(f"‚ùå Error: {result['error']}")
        else:
            print(f"üí¨ Result: {result.get('result', 'No result')[:200]}...")
            
    await service.cleanup()
    print("\n‚úÖ LLM Integration test completed")

if __name__ == "__main__":
    # –î–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –±–µ–∑ —Ä–µ–∞–ª—å–Ω—ã—Ö API –∫–ª—é—á–µ–π
    asyncio.run(test_llm_integration())