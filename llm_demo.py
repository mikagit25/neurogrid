#!/usr/bin/env python3
"""
NeuroGrid LLM Demo - —Ç–µ—Å—Ç–∏—Ä—É–µ–º –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é LLM –º–æ–¥–µ–ª–µ–π
"""

import requests
import json
import time

COORDINATOR_URL = "http://localhost:8080"

def test_llm_generation():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —á–µ—Ä–µ–∑ LLM API"""
    print("ü§ñ Testing NeuroGrid LLM Generation")
    print("=" * 50)
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
    test_requests = [
        {
            "prompt": "Explain what is NeuroGrid and how decentralized AI computing works",
            "model": "llama2-7b",
            "max_tokens": 500
        },
        {
            "prompt": "Write a Python function to calculate fibonacci numbers",
            "model": "codellama-7b",
            "provider": "local"
        },
        {
            "prompt": "Create a haiku about artificial intelligence",
            "model": "gpt-3.5-turbo",
            "temperature": 0.9
        }
    ]
    
    headers = {
        'Content-Type': 'application/json',
        'User-Agent': 'NeuroGrid-LLM-Demo/1.0'
    }
    
    for i, request_data in enumerate(test_requests, 1):
        print(f"\n{i}Ô∏è‚É£ Testing request: '{request_data['prompt'][:50]}...'")
        print(f"   Model: {request_data.get('model', 'auto')}")
        print(f"   Provider: {request_data.get('provider', 'auto')}")
        
        try:
            start_time = time.time()
            response = requests.post(
                f"{COORDINATOR_URL}/api/llm/generate",
                json=request_data,
                headers=headers,
                timeout=30
            )
            total_time = time.time() - start_time
            
            if response.status_code == 200:
                result = response.json()
                if result['success']:
                    data = result['data']
                    print(f"   ‚úÖ Success!")
                    print(f"   ‚è±Ô∏è  Total time: {total_time:.2f}s")
                    print(f"   üî• Processing time: {data.get('processing_time', 0):.2f}s") 
                    print(f"   üè∑Ô∏è  Model used: {data.get('model', 'unknown')}")
                    print(f"   üñ•Ô∏è  Node: {data.get('node_id', 'unknown')}")
                    print(f"   üí∞ Cost: {data.get('cost_neuro', 0)} NEURO (${data.get('cost_usd', 0)})")
                    print(f"   üî§ Tokens: {data.get('tokens_used', 0)}")
                    print(f"   üí¨ Result: {data.get('result', 'No result')[:150]}...")
                else:
                    print(f"   ‚ùå API Error: {result.get('error', 'Unknown error')}")
            else:
                print(f"   ‚ùå HTTP Error: {response.status_code}")
                print(f"   Response: {response.text}")
                
        except requests.exceptions.Timeout:
            print(f"   ‚è∞ Request timeout after 30s")
        except Exception as e:
            print(f"   ‚ùå Error: {e}")
            
    print(f"\nüéâ LLM Demo completed!")

def test_available_models():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –ø–æ–ª—É—á–µ–Ω–∏–µ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
    print("\nüìã Testing Available Models API")
    print("-" * 30)
    
    try:
        response = requests.get(f"{COORDINATOR_URL}/api/llm/models")
        if response.status_code == 200:
            result = response.json()
            if result['success']:
                data = result['data']
                print(f"üìä Total models: {data.get('total_models', 0)}")
                
                for category, models in data.get('available_models', {}).items():
                    print(f"\nüè∑Ô∏è  {category.replace('-', ' ').title()}:")
                    for model in models:
                        cost_info = f"${model['cost']}" if model['cost'] > 0 else "Free"
                        print(f"   ‚Ä¢ {model['name']} ({model['provider']}) - {cost_info}")
            else:
                print(f"‚ùå API Error: {result.get('error', 'Unknown error')}")
        else:
            print(f"‚ùå HTTP Error: {response.status_code}")
            
    except Exception as e:
        print(f"‚ùå Error: {e}")

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–µ–º–æ"""
    print("üöÄ NeuroGrid LLM Integration Demo")
    print("=" * 60)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å API
    try:
        response = requests.get(f"{COORDINATOR_URL}/health", timeout=5)
        if response.status_code == 200:
            health = response.json()
            print(f"‚úÖ NeuroGrid API is healthy")
            print(f"üìä Service: {health.get('service', 'Unknown')}")
            print(f"üïê Uptime: {health.get('performance', {}).get('uptime', 0)}s")
        else:
            print("‚ùå NeuroGrid API is not responding properly")
            return
    except:
        print("‚ùå Cannot connect to NeuroGrid API. Make sure enhanced-server is running.")
        return
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å
    test_available_models()
    test_llm_generation()
    
    print("\n" + "=" * 60)
    print("üéØ Next Steps:")
    print("1. Add real API keys to test OpenAI/HuggingFace integration")
    print("2. Install Ollama locally to test local models")
    print("3. Start node simulator to see distributed processing")
    print("4. Check web interface at http://localhost:3000")
    print("\nüí° To start node simulator: python node_simulator.py")

if __name__ == "__main__":
    main()